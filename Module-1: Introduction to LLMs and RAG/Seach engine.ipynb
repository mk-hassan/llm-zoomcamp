{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b31272cf-c3b0-4805-9dd3-4f0f1f6d2b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-06-24 09:59:19--  https://raw.githubusercontent.com/alexeygrigorev/minsearch/main/minsearch.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3832 (3.7K) [text/plain]\n",
      "Saving to: ‘minsearch.py’\n",
      "\n",
      "minsearch.py        100%[===================>]   3.74K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-06-24 09:59:19 (12.3 MB/s) - ‘minsearch.py’ saved [3832/3832]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/alexeygrigorev/minsearch/main/minsearch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbb2dd6a-4872-4a3e-bf89-01d15d1853ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-06-24 10:02:36--  https://raw.githubusercontent.com/DataTalksClub/llm-zoomcamp/main/01-intro/documents.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 658332 (643K) [text/plain]\n",
      "Saving to: ‘documents.json’\n",
      "\n",
      "documents.json      100%[===================>] 642.90K  --.-KB/s    in 0.01s   \n",
      "\n",
      "2024-06-24 10:02:36 (47.5 MB/s) - ‘documents.json’ saved [658332/658332]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/DataTalksClub/llm-zoomcamp/main/01-intro/documents.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "192de74c-d42b-43f8-a310-e13a581bd1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "from groq import Groq\n",
    "from minsearch import Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d334f80a-21d2-4b19-879f-430f3672b7bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('documents.json', 'rt') as f_in:\n",
    "    courses = json.load(f_in)\n",
    "\n",
    "len(courses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86ce5a91-b2fa-4d69-8877-7c46f761093b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "948"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = []\n",
    "\n",
    "for course in courses:\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course['course']\n",
    "        documents.append(doc)\n",
    "\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f75da5dc-9dcc-4783-a87c-f1093ade9624",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = Index(\n",
    "    text_fields=['text', 'section', 'question'],\n",
    "    keyword_fields=['course']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73da6331-c7aa-46c2-832c-5b69d4ff0f3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.Index at 0x74f3ccba3250>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15f43a44-d87e-4d07-935f-c6f076b80d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    return engine.search(query, \n",
    "                         filter_dict={\"course\": \"data-engineering-zoomcamp\"}, \n",
    "                         boost_dict = {'question': 3.0, 'section': 0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "295325b0-1e68-4074-92c1-5f141b8dcf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT: \n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3a3b9716-395f-4111-9e9c-6402498b2740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_groq(prompt, model=\"mixtral-8x7b-32768\"):\n",
    "    client = Groq(\n",
    "        api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    "    )\n",
    "    \n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "96c967f1-96fe-4375-b246-3d2c284285bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_llm(q):\n",
    "    docs = search(q)\n",
    "    prompt = build_prompt(q, docs)\n",
    "    return ask_groq(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c9d185a-3de3-49f4-bc24-01ce87aea406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'section: Module 6: streaming with kafka\\nquestion: Java Kafka: How to run producer/consumer/kstreams/etc in terminal\\nanswer: In the project directory, run:\\njava -cp build/libs/<jar_name>-1.0-SNAPSHOT.jar:out src/main/java/org/example/JsonProducer.java\\n\\nsection: Module 6: streaming with kafka\\nquestion: Module “kafka” not found when trying to run producer.py\\nanswer: Solution from Alexey: create a virtual environment and run requirements.txt and the python files in that environment.\\nTo create a virtual env and install packages (run only once)\\npython -m venv env\\nsource env/bin/activate\\npip install -r ../requirements.txt\\nTo activate it (you\\'ll need to run it every time you need the virtual env):\\nsource env/bin/activate\\nTo deactivate it:\\ndeactivate\\nThis works on MacOS, Linux and Windows - but for Windows the path is slightly different (it\\'s env/Scripts/activate)\\nAlso the virtual environment should be created only to run the python file. Docker images should first all be up and running.\\n\\nsection: Workshop 1 - dlthub\\nquestion: How do I install the necessary dependencies to run the code?\\nanswer: Answer: To run the provided code, ensure that the \\'dlt[duckdb]\\' package is installed. You can do this by executing the provided installation command: !pip install dlt[duckdb]. If you’re doing it locally, be sure to also have duckdb pip installed (even before the duckdb package is loaded).\\n\\nsection: Module 6: streaming with kafka\\nquestion: Python Kafka: ./build.sh: Permission denied Error\\nanswer: Run this command in terminal in the same directory (/docker/spark):\\nchmod +x build.sh\\n\\nsection: Project\\nquestion: How to fix the error \"ModuleNotFoundError: No module named \\'kafka.vendor.six.moves\\'\"?\\nanswer: According to https://github.com/dpkp/kafka-python/\\n“DUE TO ISSUES WITH RELEASES, IT IS SUGGESTED TO USE https://github.com/wbarnha/kafka-python-ng FOR THE TIME BEING”\\nUse pip install kafka-python-ng instead\\n\\nsection: Module 6: streaming with kafka\\nquestion: kafka.errors.NoBrokersAvailable: NoBrokersAvailable\\nanswer: If you have this error, it most likely that your kafka broker docker container is not working.\\nUse docker ps to confirm\\nThen in the docker compose yaml file folder, run docker compose up -d to start all the instances.\\n\\nsection: Module 6: streaming with kafka\\nquestion: Python Kafka: ./spark-submit.sh streaming.py - How to check why Spark master connection fails\\nanswer: Start a new terminal\\nRun: docker ps\\nCopy the CONTAINER ID of the spark-master container\\nRun: docker exec -it <spark_master_container_id> bash\\nRun: cat logs/spark-master.out\\nCheck for the log when the error happened\\nGoogle the error message from there\\n\\nsection: Module 6: streaming with kafka\\nquestion: How do I check compatibility of local and container Spark versions?\\nanswer: You can check the version of your local spark using spark-submit --version. In the build.sh file of the Python folder, make sure that SPARK_VERSION matches your local version. Similarly, make sure the pyspark you pip installed also matches this version.\\n\\nsection: Module 6: streaming with kafka\\nquestion: Python Kafka: Installing dependencies for python3 06-streaming/python/avro_example/producer.py\\nanswer: confluent-kafka: `pip install confluent-kafka` or `conda install conda-forge::python-confluent-kafka`\\nfastavro: pip install fastavro\\nAbhirup Ghosh\\nCan install Faust Library for Module 6 Python Version due to dependency conflicts?\\nThe Faust repository and library is no longer maintained - https://github.com/robinhood/faust\\nIf you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80  and follow the RedPanda Python version here https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example - NOTE: I highly recommend watching the Java videos to understand the concept of streaming but you can skip the coding parts - all will become clear when you get to the Python videos and RedPanda files.\\n\\nsection: General course-related questions\\nquestion: How do I use Git / GitHub for this course?\\nanswer: After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\\nHaving this local repository on your computer will make it easy for you to access the instructors’ code and make pull requests (if you want to add your own notes or make changes to the course content).\\nYou will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\\nRemember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\\nThis is also a great resource: https://dangitgit.com/\\n\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_llm(\"How do I run kafka?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b8e48f95-807b-4cb3-89bb-b14ac1f1fded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# elastic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "403d77b0-a9a7-409b-b86a-9753e2e0a21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad51388c-0a9f-4d9c-a20a-55c413d6b476",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_client = Elasticsearch('http://localhost:9200') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea9690e-c016-4559-82f8-b1f8510ebaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"section\": {\"type\": \"text\"},\n",
    "            \"question\": {\"type\": \"text\"},\n",
    "            \"course\": {\"type\": \"keyword\"} \n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "index_name = \"course-questions\"\n",
    "\n",
    "es_client.indices.create(index=index_name, body=index_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3cdccd52-9b11-4ff8-8bf7-6cb13c699c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 948/948 [00:31<00:00, 30.38it/s]\n"
     ]
    }
   ],
   "source": [
    "for doc in tqdm(documents):\n",
    "    es_client.index(index=index_name, document=doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f21d28eb-4a05-43ac-accd-4d6621421d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_search(query):\n",
    "    search_query = {\n",
    "        \"size\": 5,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\": query,\n",
    "                        \"fields\": [\"question^3\", \"text\", \"section\"],\n",
    "                        \"type\": \"best_fields\"\n",
    "                    }\n",
    "                },\n",
    "                \"filter\": {\n",
    "                    \"term\": {\n",
    "                        \"course\": \"data-engineering-zoomcamp\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = es_client.search(index=index_name, body=search_query)\n",
    "    \n",
    "    result_docs = []\n",
    "    \n",
    "    for hit in response['hits']['hits']:\n",
    "        result_docs.append(hit['_source'])\n",
    "    \n",
    "    return result_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9d2e8835-9d01-4aac-b8b7-8b6003ce6272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_ask_llm(q):\n",
    "    docs = elastic_search(q)\n",
    "    prompt = build_prompt(q, docs)\n",
    "    return ask_groq(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "76da6b8c-c894-4f99-9029-f9f73f6f9ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To run Kafka, you need to follow the instructions provided in the \"Module 6: streaming with kafka\" section of the FAQ database. If you're encountering an issue with the \"module 'kafka' not found\" error, you should create a virtual environment and run the requirements.txt and python files in that environment.\n",
      "\n",
      "Here are the steps to create a virtual environment and install the necessary packages:\n",
      "\n",
      "1. Open a terminal and navigate to the project directory.\n",
      "2. Create a virtual environment by running: `python -m venv env`\n",
      "3. Activate the virtual environment with the command: `source env/bin/activate` (on Windows, use: `env\\Scripts\\activate`)\n",
      "4. Install the required packages with: `pip install -r ../requirements.txt`\n",
      "\n",
      "After setting up the virtual environment, you can run the Kafka producer/consumer/kstreams by using the Java commands provided in the FAQ database. For example, to run the producer, navigate to the project directory and execute:\n",
      "\n",
      "```\n",
      "java -cp build/libs/<jar_name>-1.0-SNAPSHOT.jar:out src/main/java/org/example/JsonProducer.java\n",
      "```\n",
      "\n",
      "Remember to replace `<jar_name>` with the actual name of your JAR file.\n",
      "\n",
      "For checking compatibility of local and container Spark versions, use `spark-submit --version` to find the local version. Make sure the SPARK_VERSION in the build.sh file matches your local version, and that the pyspark version installed with pip also matches.\n"
     ]
    }
   ],
   "source": [
    "elastic_ask_llm(\"How do I run kafka?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ef90080-94c2-4ff5-9001-aa3a140d5a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_client.options(ignore_status=[400,404]).indices.delete(index='course-questions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974a9a76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
